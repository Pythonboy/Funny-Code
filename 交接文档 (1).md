本文观看说明：先是介绍了所有用到的表格，然后介绍了本人参与的6个项目，最后是各个项目用到的代码的使用方法及位置，方便后人使用和优化。通过本文可以直接了解本人做的所有内容和代码位置使用等。
**前提介绍：**
我们拥有的数据是个推的数据，大致可以分为两类：一类是线上数据，一类是线下数据。

*线上数据：*

 - xxx设备（gid）在xxx时间使用了xxx app（该数据主要在type24中）；
 - xxx设备在该月的属性（包括性别、年龄、职业、所在地、消费水平等等（个推打的标签））（该数据主要在wide表中）
 - xxx设备的的imsi，imei，分别是手机的识别号和sim卡的卡槽识别号（忘记在哪个表里了）

 *线下数据：*
 -gid在何时（某一天）在何地（geohash）活跃了一次（被检测到了一次）（在lbs表中）

*其他：*
-调整比例表：由于个推公司的发展，有时候会新接入一些app，此时捕捉到的手机数量大大增多，会产生一个突变，此外由于各种算法的调整也会导致采集到的有很大的变化，因此用一个系数进行调整，系数获得是通过省市的公开数据人数与个推公司采集人数相除得到的系数。（从上述获得系数的方法可以知道，该系数会造成局部不均匀，因为他是从一个省市的宏观角度进行调整的）。*该表我主要是用来对某区域的人流强度进行调整。下表中的adjust那个列就是调整系数，logday是个推的更新日*

## 项目一：区域人流强度变化分析
这是我做的第一个项目，相必也是为了让我练练手吧，非常简单，思路和操作都简单。

 1. 在地图上圈出所需查找地点的范围，找到其geohash；(这里用到了个推提供的工具，geohash生成器）
 2. 将geohash送到个推，上传到服务器（因为我们无法将外网的数据传到服务器上），然后利用代码（read）导入到hive中；，此处需要给geohash所在的表取个名字；
 3. 从libs表中（数据库中的数据的线下数据）搜索出所需查找的地点范围的数据，建立新表（取个名字）来存放这些数据；
 4. 搜索出所需范围的人流量（包括不去重和去重两种），搜索所在省市的人流量（包括去重和不去重两种），相除得到ratio；
 5. 做出day—ratio图和week-ratio图（作图十分简单了，把数据从hive中下载下来，存为csv，然后用python画一下这样子）；
 这里放两个图吧：
 第一个是海航实业的人流强度变化折线图：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190509234640833.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)
**人流强度变化折线图突变解决方案**
绘制出了十余个地区的人流天数变化折线图，各个省市的都有，然后观察各个地区的人流天数变化折线图的突变情况，找到每个地区或是大部分地区都出现的突变点，这些突变时间点就认为是由于个推算法改变或是新app接入所带来的突变，在确定了这些突变点后便用以下简单到极点的方法作调整：
以突变点为时间断点，找到突变点后的N天，突变点前的N天，然后用突变后的N天的平均人流强度除以突变前N天的平均人流强度，得到我们的调整系数，然后对该区域的人流强度进行调整，将其拉平，直观效果见下面的两张图。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190509235354706.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190509235744603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)
上面两张图除了进行了上述算法的调整，将突变点拉平之外，还进行了另一个调整，也就是2017年春节期间的调整。
这个问题是在做出整个省市的人流强度变化折线图的过程中发现的问题，18年的春节3、4线城市的人数大大增多，1/2线城市的人数大大下降，而17年春节却无此现象，和个推沟通后发现是因为17年他们没有进行考虑春节效应的调整系数的计算，而18年是考虑到了这个问题。
因此在做某区域人流天数变化折线图的时候需要对17年的春节进行调整，因为17年的春节的春节效应有可能不出现。
**调整算法如下所示：**
首先找到18年春节的人流天数变化折线图的最低点，然后以改时间点为中心位置，向两边逐天遍历，计算各天与最低点的比例，形成一个比例数组。然后定位到17年开始下降的时间点，基本上是与18年开始的时间相同的（比如都定位过年前x天），然后从该天开始遍历，后续每一天都乘上这个比例数组对应的比例，这样相当于把18年的春节效应复制到了17年，也就调整完毕了。

## 项目二：app日活变化折线图
前提：利用的是何博整理的放在mysql中的app表格，该table中的数据是某app每日的活跃人数。
绘制方法：将mysql中的目标app的数据select出来，然后将目标app的日活除以微信的日活（相当于将微信的日活作为基础），然后直接绘制出app的日活天数变化折线图。
结果如下所示：（这里以东兴证券日活变化折线图）
![在这里插入图片描述](https://img-blog.csdnimg.cn/201905101141365.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)

## 项目三：各app的人均使用时长、人均启动次数筛选
前期介绍：这里用到了type24表格，type24中的数据是某个gid在何时在何地活跃了xxx and xxx·····app共多少时间（s）。
算法思路：计算每行的app个数，记为n，然后用总使用时间除以n，得到每个app的使用时间；启动次数即为该app所在的行数，然乎分别筛选出来得到一个表格。得到的表格形式为：
*人均使用时常进行了算法调整：*
查阅资料，微信的人均使用时常约为82min，以此为基准，对其余app的人均使用时常进行调整。
![人均使用时常](https://img-blog.csdnimg.cn/20190510154146888.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)
![人均启动次数](https://img-blog.csdnimg.cn/20190510154210855.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)
## 项目四：app/区域各属性人群日活变化折线图/比例变化折线图/相对比例变化折线图
这个项目相对比较大，  过程比较复杂，整个过程的算法如下所示：
*前提介绍：*
用到了3张表：1、type24（某gid在xxx日使用了xxx and xxx····app）；2、sample表（何博做出来的信息较为完整可信的sample）；3、gid_attribute（某个gid的属性，如性别，年龄消费水平，居住地点等等）

**app各属性人群日活变化折线图/比例变化折线图/相对比例变化折线图**
 1. 利用sql代码，得到某个gid，使用了xxx app，该gid的属性是xxx（如男，高消费水平，一线城市等等）的*table1*；
 2. 利用sql代码对所有的的天的整理好的*table1*进行第2步骤的整理，创建出某一天的各个属性的人数的表格，如男的400日活，女的200日活，一线300日活······，得到*table2*；
 3. 利用所有天的*table2*聚合信息得到*table3*，该表为20180105的所有需要的tag的日活，20180112所有需要的tag的日活······得到某app需要的信息后下载到本地csv。
 4. 该步使用前一步得到的csv文件和前期得到的属性人数调整文件（属性日活突变调整算法详见后述）和sample文件（整个样本池的各属性比例）以及整个样本池的的属性人数调整文件（整个sample池的日活突变调整时间点以及方法都与app的属性日活调整算法是相同的）。利用python脚本做出3样结果：1、xxx app的各个属性的日活变化折线图（这个结果不好，一般不理会，只是辅助看看）；2、xxx app的各个属性的比例变化折线图（也就是各个二级属性所占的比例的变化折线图，如男的占男女总数的变化折线图，女的占男女总数的变化折线图）；3、xxx app的各个属性的相对比例变化折线图（也就是app的各个二级属性的比例与整个sample池的对应的二级属性的比例的比值），该值可以与1对比，>1意味着在该群体的流行度很高，=1说明与大盘一样，<1说明流行度很低，此外各个二级属性也可以对比,比如男的为1.4，女的为0.4，说明男的用户中更倾向于使用该app。
 5. 区域的各属性人群比例的筛选办法是与app的各属性的比例筛选办法是相同的，在此不再赘述了。

**前面的提到的属性突变调整算法：**
由于我们不可以使用未来数据，因此得到xxx app xxx属性的日活变化折线图时是只用上个月的sample表（样本池），上个月的gid标签表，以及本月的线上数据进行筛选，得到本月xxx app各个属性的日活。
而突变产生的原因，我们通常认为是由于个推的标签算法进行调整所导致的，即gid_attribute表发生了变化，相同的gid可能在前后两个月里不再是同一个标签了，于是我们用上个月sample表，本月的gid_attribute,以及本月的线上数据（type24）进行筛选，得到一个adj_data。
于是使用突变前一天的adj_data的某属性日活/not_adj_data的某属性日活，得到该属性的调整系数，然后将该属性突变前所有时间的数据均乘上这个系数，就对该突变进行了调整。
最后展示一个app的日活、属性比例变化折线图、相对属性比例变化折线图
![男女群体的日活变化折线图](https://img-blog.csdnimg.cn/20190510154922834.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)
![男女比例变化折线图](https://img-blog.csdnimg.cn/20190510155226464.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)
 ![男女相对比例](https://img-blog.csdnimg.cn/20190510160630804.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)
## 项目五：中国各省市按照经济发展程度划分为：发达、中等、欠发达/龙虎板数据中前1000位的营业部的筛选/各经济程度省市的龙虎板数据（买入金额、卖出金额）
**中国省份的经济发达程度划分：**
利用省市的5个数据，人均可支配收入、gdp、财政预算、人口数量、人口密度，利用kmeans聚类成4类，因为北京、上海的情况过于特殊，因此需要多聚一类，北京上海会自成1类。（十分简单，不再叙述）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190510144320563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)
**龙湖版数据前1000位的营业部的筛选/各经济程度省市的龙虎板数据**
首先筛选出各个营业部一年的买入总金额，然后进行排序就可以得到前1000位的营业部了。剔除掉无用的营业部，仅仅对前1000位的营业部分析。分别得到了前1000位营业部的每日买入总额及卖出总金额总数，前1000位营业部的发达省市的买入和卖出金额总数，各个省市的买入和卖出总数（由于都是些直观简单的统计分析，就不再详述了）

## 项目六：将四维数据（龙虎板数据、金融类app安装量、金融类app日活、金融类app的人均使用时长）做PCA合成一维数据 
该项目是前期几个项目的综合：
该项目的总思路是将4个维度的数据一一得到，然后将4个维度的数据合并，得到一个dataframe，然后作为input，利用PCA降至1维。

 1. 龙湖版数据获取，该数据就是*项目五*中的每日的前1000位营业部的买入金额+卖出金额，每个交易日一个数据；
 2. 金融类app安装量，该数据是利用何博前期做的app_install中的数据，每月有一个某app的净安装量，然后将目标app的净安装量全部加总；
 3. 金融类app日活，该数据就是*项目二*中的数据，将目标app的日活加总起来；
 4. 金融类app的人均使用时长，该数据就是*项目三*的数据，将目标app的数据筛选出来，然后按照day groupby一下，得到目标的ddataframe。
 5. 将之前4步得到的数据全部获取后，以app日活数据为基准数据，因为该数据的天数是最多最完整的，然后使用日期补全算法（日期补全算法详见下述）对其余3种数据补全，然后按照day做表连接，得到最终的dataframe数据。
 6. 得到最终数据之后，将dataframe转化为4维list后作为input，用pca对象训练后，输出一维数据。
 **日期补全算法：**
 首先以日活数据为基准，其余数据均与这个数据做左连接。然后其余3种数据都会有NAN值，对于有缺失的数据就取出来，进行遍历，遇到NAN值，就开始往右边寻找最接近的值来做补全，若此过程失败，就开始往左边去寻找，找到最接近的值来做补全。
最后利用PCA得到的那个综合指标变化折线图如下所示：
![pca得到的综合指数变化折线图](https://img-blog.csdnimg.cn/20190510160833605.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5ODA1MzYy,size_16,color_FFFFFF,t_70)

## 代码使用说明
**项目一代码使用/区域人流强度变化分析**
*代码位置：'ywb代码'->'人流强度变化折线图'->'建表文件'*
1、将geohash上传后，使用create table代码，只需要修改表名就行了；
2、从lbs中筛选出需要的数据，其中有所有公司的数据都在一个表里，然后分开来就行了。

*代码位置：'ywb代码'->'人流强度变化折线图'->'区域人流强度调整及控制'*
input：需要在代码所在文件夹放置两个文件：
1、beijing_rate ；(从pops（个推的调整系数表)中筛选出来的北京的调整系数文档）；
2、haihang_dist_result；海航实业每日的人流强度数据；
output：1、海航实业7日滚动平均变化折线图；2、海航实业日活强度数据；3、海航实业调整后日频变化折线图；4、海航实业周频变化折线图；
代码function：将海航实业的日活*调整系数，再将突变调整后，绘制折线图，详见前述。

**项目二代码使用/日活变化折线图**
*代码位置：'ywb代码'->'app日活变化折线图'->'各app日活变化情况分析'*
input:NULL
output:所有目标app的日活变化折线图
代码function：从mysql的app_newxxxx 中将数据读出来，并整理成dataframe，然后绘制各个app的日活变化折线图，详见前述。
note：使用该代码时需要修改代码中目标app名字（豌豆荚中的网址），然后绘制目标app的折线图的代码也需要做对应的修改。

**项目三代码使用/各app的人均使用时长、人均启动次数筛选**
*代码位置：'ywb代码'->'app使用时常和次数'->'app使用时常和次数筛选'*
sql代码：
function：
筛选出所有目标app在目标日的平均使用时长、总使用时长、使用人数、平均启动次数，总启动次数。
note：每次使用该代码需要修改目标app的名字，目标的日期，建成表的表名。

*代码位置：'ywb代码'->'app使用时常和次数'->'某app所有时间的日均启动次数使用时长筛选'*
sql代码：
function：筛选出某app所有时间的的平均使用时常和启动次数
note:需要修改app网址

**项目四代码使用/app/区域各属性人群日活变化折线图/比例变化折线图/相对比例变化折线图**
*代码位置：'ywb代码'->''属性比例->'app属性数据筛选'*
sql代码：
function：筛选出某个目标日某个app各个gid的属性
note：需要修改目标app的名字，目标日，属性表（e.g  gid_attribute_1803)(上一个月的属性表），样本池表（gid_sample_1802)(上一个月的样本池），建成表的表名；

*代码位置：'ywb代码'->''属性比例->'app筛选出整理好的结果'*
sql代码：
function：将上一个代码建成的表进一步处理，生成中间表，该日的目标app的各个属性的日活
note：这里用到了上一个代码建成的表需要修改其表名，建成表的表名；

*代码位置：'ywb代码'->''属性比例->'某app结果获取'*
sql代码：
function：将所有时间的上一步生成的表的数据聚合起来，得到目标app的数据
note：这里用到了上一个代码建成的表需要修改其表名，目标app的代码；

*代码位置：'ywb代码'->''属性比例->'华尔街见闻->''用户受众分析*
input：‘get_sample'(全样本数据获取代码)+'sample_attribute_xxxx'(全样本数据）+’hejjw‘（目标app的属性数据，利用上一步代码获得）+’middle_adj_xxxxxx'（属性调整表数据，详见前述）
output：目标app的各属性活跃变化折线图+目标app的各属性比例变化折线图+目标app的各属性相对比例变化折线图
function：将目标app属性的活跃绘制成折线图，目标app的各属性比例计算及绘制，目标app的各相对比例计算及绘制

**项目五/中国各省市按照经济发展程度划分为：发达、中等、欠发达/龙虎板数据中前1000位的营业部的筛选/各经济程度省市的龙虎板数据（买入金额、卖出金额）**
一般不再用到，且只是直接的统计分析，不再放代码

**项目六/将四维数据（龙虎板数据、金融类app安装量、金融类app日活、金融类app的人均使用时长）做PCA合成一维数据 **
*代码位置：'ywb代码'->''pca获取指标>'整个股市的->''指标pca*
input：'get_allapp_activenum'(获取所有app的日活的和）+'get_allapp_avgtime'（获取所有app的平均使用时长的和）+‘get_allapp_install_num'（获取所有app安装量数据）+'get_stock_market_data'（获取龙虎板的数据）+’app_avgtime（csv文件，所有app的平均启动时长）+‘Turnover_byday'(龙虎板的数据）
output：一维的pca降维后的数据
function：该代码将4个输入指标降维至一维。
note：其中4个指标的获取代码及方式可以看前面。
 
